ğŸ“„ RAG PDF Q&A Bot (Gemini + Groq + FastAPI) : 

Ask questions about "Your own PDF" and get answers with page references.

This project:

- Lets you "upload a PDF"
- Breaks it into small chunks
- Turns each chunk into "vectors" (numbers) using "Google Gemini embeddings"
- Stores them in a "FAISS" vector database
- When you ask a question , it:
  - finds the most relevant chunks
  - sends them (as context) + your question to ""Groq Llama 3.1""
  - shows the answer + which pages it used

You can run it:

- on "Your own laptop" (localhost)
- and optionally share it over the internet using "ngrok" for free
  

Tech Stack (Tools we used and why)

BACKEND :

- Python â€“ main programming language
- FastAPI â€“ web framework for building APIs (`/upload`, `/ask`, and serving the website)
- Uvicorn â€“ runs the FastAPI app (the actual server)
- PyMuPDF (fitz) â€“ reads PDF files and extracts text page-by-page
- FAISS (faiss-cpu) â€“ vector database from Facebook/Meta; stores embeddings and lets us search â€œsimilarâ€ chunks fast
- Pandas â€“ stores metadata about chunks (page number, text, etc.) in a table
- Tiktoken â€“ helps with token/length management when chunking text
- Requests â€“ sends HTTP requests to Gemini and Groq APIs

AI / LLMs :

- Google Gemini Embedding API  
  - Used only for embeddings  
  - Converts text (chunks + questions) into vectors (lists of numbers)
- Groq Llama 3.1 (llama-3.1-8b-instant)  
  - Used for " chat / answering questions " 
  - Reads the context (top chunks) + question and generates the final answer

Frontend

- Plain HTML + CSS + JavaScript (`frontend/index.html`)
  - Simple web page:
    - Upload PDF
    - Ask question
    - See chat-style answers + page references

Dev / Other

- Git + GitHub â€“ version control and hosting the repo
- ngrok (optional) â€“ creates a public URL that points to your local server so others can use it
  

What is RAG ? 

RAG = Retrieval-Augmented Generation.

Instead of asking an LLM directly:

  â€œWhatâ€™s inside my PDF ?â€

We do two steps:

1. Retrieval  
   - Break PDF into pieces  
   - Embed each piece into a vector  
   - When you ask a question , embed the question and search for the most similar pieces  
   - Get those top pieces (â€œcontextâ€)

2.  Generation  
   - Give that "Context + question" to an LLM  
   - LLM generates answer based only on that context

So the model doesnâ€™t â€œGuessâ€ from general internet ; it answers using "Your uploaded document".


Project Structure

```text 
rag-app/
â”œâ”€ backend/
â”‚  â”œâ”€ main.py            # FastAPI app: routes /, /upload, /ask
â”‚  â”œâ”€ gemini_client.py   # Talks to Gemini (embeddings) and Groq (chat)
â”‚  â”œâ”€ chunking.py        # Extracts text from PDF and splits into chunks
â”‚  â”œâ”€ faiss_store.py     # Creates + loads FAISS index, saves metadata
â”‚  â””â”€ ... (other helpers if any)
â”‚
â”œâ”€ frontend/
â”‚  â””â”€ index.html         # Simple web UI (upload + ask)
â”‚
â”œâ”€ uploads/              # PDFs uploaded (created at runtime)
â”œâ”€ vector_store/         # FAISS index + metadata per document (created at runtime)
â”œâ”€ requirements.txt      # Python dependencies
â””â”€ README.md             # This file
